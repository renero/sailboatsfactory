{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with Multivariate LSTM and indices...\n",
        "\nI'm reading a datafile with all the REPSOL indices that we were computing using the 9th floor Spark processes. We will try to predict if the vlaue will go 'Up' or 'Down'."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv, concat, DataFrame\n",
        "from matplotlib import pyplot\n",
        "from numpy import concatenate\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the input dataset\n",
        "\n",
        "    Input <-  Path\n",
        "    Output -> raw_dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(file_path):\n",
        "    columNames = ['price','var-1','var-2','var-3','var-4','var-5','var-6','var-7','var-8',\n",
        "                 'var-9','var-10','var-11','var-12','var-13','var-14','var-15']\n",
        "    \n",
        "    raw_dataset = read_csv(file_path, header='infer', delimiter=';', usecols=columNames)\n",
        "    # Remove the first column as it contains the value we want to predict\n",
        "    # dataset = raw_dataset.iloc[:, 1:]   \n",
        "    return(raw_dataset)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Series-To_Supervised\n",
        "\nThis function is KEY as it produces the array shaped with t-n look-back samples to feed the LSTM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale > Reframe > Drop\n",
        "\nLet's make things reproducible. We also convert anything that might not be a float to `float32`. Data in NN is normalized to produce equivalent responses in the different layers. We also do that in this chunk. Then, data is scaled to the range 0..1, reframed according to the syntax in `series-to-supervised` and finally, unuseful columns are removed."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_reframe_drop(raw_dataset):\n",
        "    # Data types conversion\n",
        "    dataset = raw_dataset.astype('float32')\n",
        "\n",
        "    # Reframe\n",
        "    reframed = series_to_supervised(dataset, look_back, look_forward)\n",
        "\n",
        "    # Drop 'num_features - 1' from the tail of each row, as I only want to keep the first one, \n",
        "    # which will be what I want to predict.\n",
        "    num_features = int(reframed.shape[1] / (look_back + 1))\n",
        "    num_cols = reframed.shape[1]\n",
        "    cols_to_remove = [reframed.columns[col_idx] for col_idx in range(num_cols - num_features + 1, num_cols)]\n",
        "    prepared_dataset = reframed.drop(cols_to_remove, axis=1)\n",
        "    \n",
        "    # Scale\n",
        "    scaled = scaler.fit_transform(prepared_dataset)\n",
        "    df = DataFrame(data=scaled[:,:], index=range(0,scaled.shape[0]), \n",
        "               columns=['var-{:d}'.format(i) for i in range(scaled.shape[1])])\n",
        "\n",
        "    #return prepared_dataset\n",
        "    print('scaled & reframed shape:', df.shape)\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split in Training & Test\n",
        "\nSplit and reshape the dataset."
      ],
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test(prepared_dataset):\n",
        "    # split into train and test sets\n",
        "    values = prepared_dataset.values\n",
        "    train_size = int(round(len(values) * training_set_proportion))\n",
        "    train = values[0:train_size, :]\n",
        "    test = values[train_size:, :]\n",
        "\n",
        "    # Split into input and output.\n",
        "    train_X, train_y = train[:, :-1], train[:, -1]\n",
        "    test_X, test_y = test[:, :-1], test[:, -1]\n",
        "    \n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "    \n",
        "    return (train_X, train_y, test_X, test_y)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the LSTM !\n",
        "Define the LSTM parameters, and train it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_X, train_y, test_X, test_y):\n",
        "    # create and fit the LSTM network\n",
        "    model = Sequential()\n",
        "    if lstm_stateful is True:\n",
        "        model.add(LSTM(neurons, batch_input_shape=(lstm_batch_size, train_X.shape[1], train_X.shape[2]), stateful=True))\n",
        "    else:\n",
        "        model.add(LSTM(neurons, input_shape=(train_X.shape[1], train_X.shape[2]), stateful=False))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mae', optimizer='adam')\n",
        "    history = model.fit(train_X, train_y, epochs=num_epochs, batch_size=lstm_batch_size, \n",
        "                        validation_data=(test_X, test_y), verbose=keras_verbose_level, shuffle=lstm_shuffle)\n",
        "    return model, history\n",
        "\n",
        "def plot_model_training(history):\n",
        "    pyplot.plot(history.history['loss'], label='train')\n",
        "    pyplot.plot(history.history['val_loss'], label='test')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()    "
      ],
      "outputs": [],
      "execution_count": 179,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute the error\n",
        "\nCompute the error (RMSE) for training and test. Previously, the examples suggest to invert the results from prediction to use the same units than in the source data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def invert_Y(test_X, Y):\n",
        "    \"\"\"\n",
        "    Invert the Y vector. The way invert works requires to have a matrix with all\n",
        "    the features in place. That's why I must concatenate the test_X, so that it\n",
        "    can perform the matrix multiplication. To get only the Y, a column selection\n",
        "    is done as a final step.\n",
        "    \"\"\"\n",
        "    # Check if this Y vector is special (m,) and is not shaped correctly (m,1)\n",
        "    if len(Y.shape) is 1:\n",
        "        Y = Y.reshape((len(Y), 1))\n",
        "    test_X_reshaped = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "    # invert scaling the prediction to the original values, for forecast\n",
        "    inv_Y = concatenate((Y, test_X_reshaped[:, 0:]), axis=1)\n",
        "    inv_Y = scaler.inverse_transform(inv_Y)\n",
        "    return inv_Y[:,0]\n",
        "\n\n",
        "def predict(model, test_X, invert=True):\n",
        "    \"\"\"\n",
        "    Make a prediction with the model over the test_X dataset as input.\n",
        "    \"\"\" \n",
        "    yhat = model.predict(test_X, batch_size=lstm_batch_size)\n",
        "    if invert is False:\n",
        "        return yhat\n",
        "\n",
        "    inv_yhat = invert_Y(test_X, yhat)\n",
        "    return inv_yhat\n",
        "    \n",
        "    \n",
        "def compute_tendency_errors(Y, Yhat):\n",
        "    \"\"\"\n",
        "    Compute the error in tendency (sign of future value minus present value) when making a prediction.\n",
        "    \"\"\"\n",
        "    num_errors = 0\n",
        "    for idx in range(1, len(Y)):\n",
        "        yhat_trend = numpy.sign(Yhat[idx]-Yhat[idx-1])\n",
        "        y_trend = numpy.sign(Y[idx]-Y[idx-1])\n",
        "        error = int(yhat_trend == y_trend)\n",
        "        if error == 0:\n",
        "            num_errors += 1\n",
        "    return num_errors\n",
        "\n\n",
        "def compute_error(inv_y, inv_yhat):\n",
        "    \"\"\"\n",
        "    Compute the RMSE between the prediction and the actual values.\n",
        "    \"\"\"    \n",
        "    rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "    return rmse, compute_tendency_errors(inv_y, inv_yhat)\n",
        "\n\n",
        "def printout_errors(rmse, trend_errs, header=True):\n",
        "    if header is True:\n",
        "        print(' neurons | epochs | look_back | RMSE  | Trnd.E')\n",
        "        print('---------|--------|-----------|-------|--------')\n",
        "    print(' {:7d} | {:6d} | {:9d} | {:.03f} | {:02d}'.\n",
        "          format(neurons, num_epochs, look_back, rmse, trend_errs))"
      ],
      "outputs": [],
      "execution_count": 180,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot actual values and predicted values.\n",
        "\nPlot the whoel series, and the predicted values for the test set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_prediction(inv_y, inv_yhat, title='', timespan_length=0, num_rows=1, num_cols=1, plot_index=1):\n",
        "    # Setup the plot\n",
        "    if num_rows is 1 and num_cols is 1:\n",
        "        plt.figure(num=None, figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')\n",
        "    else:\n",
        "        plt.subplot(num_rows, num_cols, plot_index)\n",
        "    plt.gca().xaxis.grid(True, which='major', color='grey', linestyle='--')\n",
        "    plt.ylabel('price')\n",
        "    plt.title(title)\n",
        "    \n",
        "    # timespanlength is the total size of the plot, in case we want to share the\n",
        "    # plot with the training set. If set to zero, it can be easily computed as \n",
        "    # the size of the Y or Y_hat sets.\n",
        "    if timespan_length is 0:\n",
        "        timespan_length = inv_y.shape[0]\n",
        "    \n",
        "    # place the values at the end of a large array (shifted by training samples elements)\n",
        "    test_values = numpy.empty(timespan_length)\n",
        "    #test_values[:] = numpy.nan\n",
        "    test_values[-(len(inv_y)):] = inv_y\n",
        "    \n",
        "    # place the prediction as we did with test_values\n",
        "    predicted_values = numpy.empty(timespan_length)\n",
        "    #predicted_values[:] = numpy.nan\n",
        "    predicted_values[-(len(inv_yhat)):] = inv_yhat\n",
        "    \n",
        "    # Plot everything\n",
        "    plt.subplot(num_rows, num_cols, plot_index)\n",
        "    test_plot = plt.plot(test_values, marker='o')\n",
        "    plt.setp(test_plot, color='g', linewidth=3.0)\n",
        "    prediction_plot = plt.plot(predicted_values, marker='o')\n",
        "    plt.setp(prediction_plot, color='r', linewidth=1.0)\n",
        "    \n",
        "    # Print, if last plot or the only one.\n",
        "    if num_rows is 1 and num_cols is 1:\n",
        "        plt.show()"
      ],
      "outputs": [],
      "execution_count": 196,
      "metadata": {
        "collapsed": true,
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Hyper-Parameters and main pipeline function.\n",
        "\n",
        "Set here all the pipeline parameters and the main pipeline function.\n",
        "Scaler in range of (-1, +1) works better than in (0, 1)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "numpy.random.seed(2)\n",
        "file_path = '~/Documents/SideProjects/sailboatsfactory/data/ibex_1.csv'\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "lstm_stateful = True\n",
        "lstm_shuffle = True\n",
        "look_back = 10\n",
        "num_epochs = 50\n",
        "lstm_batch_size = 10\n",
        "look_forward = 1\n",
        "training_set_proportion = 0.9795\n",
        "keras_verbose_level = 2\n",
        "\n",
        "# read data\n",
        "raw_dataset = read_dataset(file_path)\n",
        "neurons = raw_dataset.shape[1]-1\n",
        "\n",
        "def run_pipeline():\n",
        "    # prepare it to fit the lstm input\n",
        "    prepared_dataset = scale_reframe_drop(raw_dataset)\n",
        "    neurons = (prepared_dataset.shape[1] - 1) * look_back\n",
        "    # split dataset intraining and test\n",
        "    train_X, train_y, test_X, test_y = split_train_test(prepared_dataset)\n",
        "    # Train the model\n",
        "    model, history = train_model(train_X, train_y, test_X, test_y)\n",
        "    plot_model_training(history)\n",
        "    # Make a prediction with the model and invert the actual test Y set.\n",
        "    inv_yhat = predict(model, test_X, invert=True)\n",
        "    inv_y = invert_Y(test_X, test_y)\n",
        "    # Compute the errors\n",
        "    rmse, trend_error = compute_error(inv_y, inv_yhat)\n",
        "    \n",
        "    return inv_y, inv_yhat, rmse, trend_error"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimal Prediction\n",
        "\nOne prediction based on 75 epochs, with look_acbk = 1 (my prediction for tomorrow will be based only in what happened today) seems the optimal one. In the title of the plot, the last term \"T.E.\" stands for Trend Errors, which means the number of times the model failed to predict if the tendency was to increase the price or lower the price."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y, yhat, rmse, trend_error = run_pipeline()\n",
        "plot_prediction(y, yhat, title='LookBack={:d}, RMSE={:.02f}, T.E={:.02f}({:d}/{:d})'.\n",
        "                format(look_back, rmse, (trend_error/(len(y)-1)), trend_error, len(y)-1))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set SIZE !!!!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = read_dataset(file_path)\n",
        "print('raw_dataset:',raw_dataset.shape)\n",
        "prepared_dataset = scale_reframe_drop(raw_dataset)\n",
        "print('prepared_dataset:',prepared_dataset.shape)\n",
        "train_X, train_y, test_X, test_y = split_train_test(prepared_dataset)\n",
        "print('train_X:',train_X.shape,', train_y:', train_y.shape)\n",
        "print('test_X:',test_X.shape,', test_y:', test_y.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw_dataset: (6860, 16)\n",
            "scaled & reframed shape: (6850, 161)\n",
            "prepared_dataset: (6850, 161)\n",
            "train_X: (6710, 1, 160) , train_y: (6710,)\n",
            "test_X: (140, 1, 160) , test_y: (140,)\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = dict()\n",
        "params['file_path'] = '~/Documents/SideProjects/sailboatsfactory/data/ibex_1.csv'\n",
        "params['scaler'] = MinMaxScaler(feature_range=(-1, 1))\n",
        "params['lstm_stateful'] = True\n",
        "params['lstm_shuffle'] = True\n",
        "params['num_epochs'] = 50\n",
        "params['lstm_batch_size'] = 10\n",
        "params['look_forward'] = 1\n",
        "params['keras_verbose_level'] = 2params['look_back'] = 2\n",
        "params['raw_numrows'] = raw_dataset.shape[0]\n",
        "params['lstm_batch_size'] = 50\n",
        "params['test_numbatches'] = 2\n",
        "\n",
        "params['raw_adjusted_numrows'] = params['raw_numrows'] - ((params['raw_numrows'] - params['look_back']) % params['lstm_batch_size'])\n",
        "params['reframed_numrows'] = (params['raw_adjusted_numrows'] - params['look_back'])\n",
        "params['train_numrows'] = params['reframed_numrows'] - (params['test_numbatches'] * params['lstm_batch_size'])\n",
        "params['test_numrows']  = (params['test_numbatches'] * params['lstm_batch_size'])\n",
        "\n",
        "# --\n",
        "\n",
        "print('raw_numrows:', params['raw_numrows'])\n",
        "print('raw_adjusted_numrows:', params['raw_adjusted_numrows'])\n",
        "print('ref_size:', params['reframed_numrows'])\n",
        "print('train_numrows:', params['train_numrows'])\n",
        "print('test_numrows', params['test_numrows'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw_numrows: 6860\n",
            "raw_adjusted_numrows: 6852\n",
            "ref_size: 6850\n",
            "train_numrows: 6830\n",
            "test_numrows 20\n"
          ]
        }
      ],
      "execution_count": 28,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.5.3",
      "nbconvert_exporter": "python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python"
    },
    "nteract": {
      "version": "0.4.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}